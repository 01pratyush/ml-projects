{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xrG3DMU9-UuT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V01Sgc4xWgBt",
        "outputId": "c371aed1-6081-4af2-800c-fa0915c5c2ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    ResNet50(include_top=False , pooling='avg' , weights = 'imagenet'),\n",
        "    keras.layers.Dense(2 , activation='softmax')\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "FzvV2gSC-i-z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf17ERlM_amS",
        "outputId": "3e149039-8173-4a88-dd6a-1fc71dce5bad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 2048)              23587712  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23591810 (90.00 MB)\n",
            "Trainable params: 4098 (16.01 KB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam' , loss = 'binary_crossentropy' , metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_O4DmtnV_2XP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/rural_and_urban_photos/train',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = 12,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    '/content/drive/MyDrive/datasets/rural_and_urban_photos/val',\n",
        "    target_size = (224 ,224) ,\n",
        "    batch_size = 20 ,\n",
        "    class_mode = 'categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FHX881gAPwx",
        "outputId": "bf8c30e1-9bf0-4305-b2bd-6cdb240b716c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 72 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator , steps_per_epoch = 6, validation_data = validation_generator,\n",
        "          validation_steps =1 , epochs = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9HL2glAXyzp",
        "outputId": "5bb2af89-2d57-48d1-ee85-1135f1cb069b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.6677 - accuracy: 0.7083 - val_loss: 0.4204 - val_accuracy: 0.9000\n",
            "Epoch 2/2\n",
            "6/6 [==============================] - 19s 3s/step - loss: 0.3124 - accuracy: 0.9167 - val_loss: 0.2383 - val_accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d7595cef820>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}